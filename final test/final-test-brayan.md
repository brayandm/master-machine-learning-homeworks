# Machine Learning for Masters Final Test

## Name: Brayan Duran Medina

### 2. What is the gradient? How is it used in optimization?

The gradient is a vector that points in the direction of the greatest increase of a function. It is used in optimization to find the minimum of a function. The gradient is used to update the parameters of a model in the direction of the greatest decrease of the loss function.

### 4. What is validation? Cross validation?

Validation is a technique used to evaluate the performance of a model. It is used to estimate how well a model will perform on new data. Cross validation is a technique used to evaluate the performance of a model by splitting the data into k folds and training the model k times, each time using a different fold as the validation set.

### 6. What are precision and recall metrics?

Precision is the ratio of true positive predictions to the sum of true positive and false positive predictions. Recall is the ratio of true positive predictions to the sum of true positive and false negative predictions.

### 8. How are parameters different from hyperparameters? E.g. what are parameters in linear models and decision trees? Hyperparameters?

Parameters are the values that a model learns from the data. They are the weights in a linear model and the split points in a decision tree. Hyperparameters are the values that are set before the model is trained. They are for example the learning rate in a linear model and the maximum depth in a decision tree, etc.

### 10. What is backpropagation? How does it work? E.g. how would gradient propagate through a linear layer? Through ReLU?

### 12. What is dropout? How does it work in a neural network? Does it change its behaviour on the inference (test) stage?

### 14. How does RNN work? Can you combine CNN and RNN somehow?
