{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Credits: materials from this notebook belong to YSDA [Practical DL](https://github.com/yandexdataschool/Practical_DL) course. Special thanks for making them available online.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab assignment â„–1, part 1\n",
    "\n",
    "This lab assignment consists of several parts. You are supposed to make some transformations, train some models, estimate the quality of the models and explain your results.\n",
    "\n",
    "Several comments:\n",
    "* Don't hesitate to ask questions, it's a good practice.\n",
    "* No private/public sharing, please. The copied assignments will be graded with 0 points.\n",
    "* Blocks of this lab will be graded separately."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Matrix differentiation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since it easy to google every task please please please try to undestand what's going on. The \"just answer\" thing will be not counted, make sure to present derivation of your solution. It is absolutely OK if you found an answer on web then just exercise in $\\LaTeX$ copying it into here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Useful links: \n",
    "[1](http://www.machinelearning.ru/wiki/images/2/2a/Matrix-Gauss.pdf)\n",
    "[2](http://www.atmos.washington.edu/~dennis/MatrixCalculus.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ex. 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$  \n",
    "y = x^Tx,  \\quad x \\in \\mathbb{R}^N \n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\frac{dy}{dx} = \n",
    "$$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\color{blue}{\\textit Your Answer:}$ \n",
    "\n",
    "\n",
    "To find the derivative of the function $y = x^T x$ with respect to $x$, where $x$ is a vector in $\\mathbb{R}^N$, we recognize that $y$ is the sum of the squares of the components of $x$. This can be expressed as:\n",
    "\n",
    "$y = x_1^2 + x_2^2 + \\cdots + x_N^2$\n",
    "\n",
    "The derivative of $y$ with respect to $x$, known as the gradient, is found by differentiating $y$ with respect to each component of $x$:\n",
    "\n",
    "$\\frac{dy}{dx} = \\left[ \\frac{\\partial y}{\\partial x_1}, \\frac{\\partial y}{\\partial x_2}, \\ldots, \\frac{\\partial y}{\\partial x_N} \\right]^T$\n",
    "\n",
    "Since the derivative of $x_i^2$ with respect to $x_i$ is $2x_i$, the gradient of $y$ with respect to $x$ is:\n",
    "\n",
    "$\\frac{dy}{dx} = [2x_1, 2x_2, \\ldots, 2x_N]^T = 2x$\n",
    "\n",
    "Therefore, the answer is:\n",
    "\n",
    "$\\frac{dy}{dx} = 2x$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ex. 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ y = tr(AB) \\quad A,B \\in \\mathbb{R}^{N \\times N} $$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\frac{dy}{dA} =\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\color{blue}{\\textit Your Answer:}$ \n",
    "\n",
    "To find the derivative of $y = tr(AB)$ with respect to $A$, where $A$ and $B$ are matrices in $\\mathbb{R}^{N \\times N}$\n",
    "\n",
    "Let's consider the trace of a matrix $C$ as the sum of the diagonal elements of $C$:\n",
    "\n",
    "Then $y = tr(AB) = \\sum_{i=1}^{N} (AB)_{ii} = \\sum_{i=1}^{N} \\sum_{j=1}^{N} A_{ij}B_{ji}$\n",
    "\n",
    "Now we can notice that for each $A_{ij}$ the only term that depends on it is $B_{ji}$, so the derivative of $y$ with respect to $A_{ij}$ is $B_{ji}$.\n",
    "\n",
    "Then for the whole matrix $A$ the derivative is:\n",
    "\n",
    "$\\frac{dy}{dA} = B^T$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ex. 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$  \n",
    "y = x^TAc , \\quad A\\in \\mathbb{R}^{N \\times N}, x\\in \\mathbb{R}^{N}, c\\in \\mathbb{R}^{N} \n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\frac{dy}{dx} =\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\color{blue}{\\textit Your Answer:}$ \n",
    "\n",
    "We have that $y = x^TAc = \\sum_{i=1}^{N} x_i(Ac)_i = \\sum_{i=1}^{N} x_i \\sum_{j=1}^{N} A_{ij}c_j$\n",
    "\n",
    "Then the derivative of $y$ with respect to $x_i$ is $\\sum_{j=1}^{N} A_{ij}c_j$.\n",
    "\n",
    "Then for the whole vector $x$ the derivative is:\n",
    "\n",
    "$\\frac{dy}{dx} = Ac$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\frac{dy}{dA} =\n",
    "$$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\color{blue}{\\textit Your Answer:}$ \n",
    "\n",
    "From the previous example  $y = x^TAc = \\sum_{i=1}^{N} x_i(Ac)_i = \\sum_{i=1}^{N} x_i \\sum_{j=1}^{N} A_{ij}c_j$\n",
    "\n",
    "Then inserting $x_i$ inside the sum we get:\n",
    "\n",
    "$y = \\sum_{i=1}^{N} \\sum_{j=1}^{N}  A_{ij}x_i c_j$\n",
    "\n",
    "Then the derivative of $y$ with respect to $A_{ij}$ is $x_i c_j$.\n",
    "\n",
    "Then for the whole matrix $A$ the derivative is:\n",
    "\n",
    "$\\frac{dy}{dA} = xc^T$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hint for the latter (one of the ways): use *ex. 2* result and the fact \n",
    "$$\n",
    "tr(ABC) = tr (CAB)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ex. 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classic matrix factorization example. Given matrix $X$ you need to find $A$, $S$ to approximate $X$. This can be done by simple gradient descent iteratively alternating $A$ and $S$ updates.\n",
    "$$\n",
    "J = || X - AS ||_F^2  , \\quad A\\in \\mathbb{R}^{N \\times R} , \\quad S\\in \\mathbb{R}^{R \\times M}\n",
    "$$\n",
    "$$\n",
    "\\frac{dJ}{dS} = ? \n",
    "$$\n",
    "\n",
    "You may use one of the following approaches:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\color{blue}{\\textit{Your Answer:}}$\n",
    "\n",
    "To find the gradient of the cost function $J = || X - AS ||_F^2$ with respect to $S$, we leverage the Frobenius norm's trace representation. This method streamlines the differentiation process by utilizing trace properties for matrix derivatives. Below is the streamlined derivation:\n",
    "\n",
    "Given:\n",
    "- $J = || X - AS ||_F^2$\n",
    "- $A \\in \\mathbb{R}^{N \\times R}$, $S \\in \\mathbb{R}^{R \\times M}$\n",
    "\n",
    "The cost function $J$ can be expressed using the trace function as follows:\n",
    "$ J = tr((X - AS)(X - AS)^T) $\n",
    "\n",
    "Upon expanding and simplifying, we obtain:\n",
    "$ J = tr(XX^T) - tr(XS^TA^T) - tr(ASX^T) + tr(AS S^TA^T) $\n",
    "\n",
    "To derive $\\frac{dJ}{dS}$, we differentiate with respect to $S$, applying trace differentiation rules:\n",
    "- The derivative of $tr(XX^T)$ is $0$, as it's constant with respect to $S$.\n",
    "- Differentiating $-tr(ASX^T)$ and $-tr(XS^TA^T)$, considering the cyclic property of trace, both contribute $-A^TX$, leading to a total of $-2A^TX$ after accounting for both terms.\n",
    "- Differentiating $tr(AS S^TA^T)$, by applying the differentiation rule $d(tr(AB))/dA = B^T$ and keeping the cyclic property in mind, yields $2A^TAS$.\n",
    "\n",
    "Therefore, the correct gradient of $J$ with respect to $S$ is:\n",
    "$ \\frac{dJ}{dS} = -2A^TX + 2A^TAS $\n",
    "\n",
    "This gradient indicates the direction for updating $S$ in a gradient descent algorithm to minimize $J$, effectively reducing the discrepancy between $X$ and its approximation $AS$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### First approach\n",
    "Using ex.2 and the fact:\n",
    "$$\n",
    "|| X ||_F^2 = tr(XX^T) \n",
    "$$ \n",
    "it is easy to derive gradients (you can find it in one of the refs). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Second approach\n",
    "You can use *slightly different techniques* if they suits you. Take a look at this derivation:\n",
    "<img src=\"grad.png\">\n",
    "(excerpt from [Handbook of blind source separation, Jutten, page 517](https://books.google.ru/books?id=PTbj03bYH6kC&printsec=frontcover&dq=Handbook+of+Blind+Source+Separation&hl=en&sa=X&ved=0ahUKEwi-q_apiJDLAhULvXIKHVXJDWcQ6AEIHDAA#v=onepage&q=Handbook%20of%20Blind%20Source%20Separation&f=false), open for better picture)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Third approach\n",
    "And finally we can use chain rule! \n",
    "let $ F = AS $ \n",
    "\n",
    "**Find**\n",
    "$$\n",
    "\\frac{dJ}{dF} =  \n",
    "$$ \n",
    "and \n",
    "$$\n",
    "\\frac{dF}{dS} =  \n",
    "$$ \n",
    "(the shape should be $ NM \\times RM )$.\n",
    "\n",
    "Now it is easy do get desired gradients:\n",
    "$$\n",
    "\\frac{dJ}{dS} =  \n",
    "$$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "## 2. kNN questions\n",
    "Here come the questions from the assignment0_01. Please, refer to the assignment0_01 to get the context of the questions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1\n",
    "\n",
    "Notice the structured patterns in the distance matrix, where some rows or columns are visible brighter. (Note that with the default color scheme black indicates low distances while white indicates high distances.)\n",
    "\n",
    "- What in the data is the cause behind the distinctly bright rows?\n",
    "- What causes the columns?\n",
    "\n",
    "$\\color{blue}{\\textit Your Answer:}$ \n",
    "\n",
    "Distinctly Bright Rows: Rows that appear brighter in the distance matrix indicate test images that are substantially different from the majority of images in the training set. This may be due to several reasons:\n",
    "\n",
    "- The test image may be an outlier or atypical with respect to the training set, possibly due to noise, a poor image capture (for example, a digit that is blurred, rotated unconventionally, or partially obstructed), or because represents a very different style of writing.\n",
    "- The test image could belong to a class that is underrepresented in the training set, causing the distances to most of the training images to be larger.\n",
    "\n",
    "Distinctly Bright Columns: Similarly, columns that appear brighter indicate images in the training set that are substantially different from the majority of images in the test set. This can also be due to several reasons:\n",
    "\n",
    "- The training image may be an outlier within its own class or among the training set in general, possibly for the same reasons mentioned above (noise, poor capture, outlier style).\n",
    "- The training image could be difficult to classify correctly even for a human, possibly due to an ambiguous or unusual representation of its corresponding digit.\n",
    "- It could indicate systematic differences between the training and test sets, although ideally these sets should be representative of each other.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2\n",
    "\n",
    "We can also use other distance metrics such as L1 distance.\n",
    "For pixel values $p_{ij}^{(k)}$ at location $(i,j)$ of some image $I_k$, \n",
    "\n",
    "the mean $\\mu$ across all pixels over all images is $$\\mu=\\frac{1}{nhw}\\sum_{k=1}^n\\sum_{i=1}^{h}\\sum_{j=1}^{w}p_{ij}^{(k)}$$\n",
    "And the pixel-wise mean $\\mu_{ij}$ across all images is \n",
    "$$\\mu_{ij}=\\frac{1}{n}\\sum_{k=1}^np_{ij}^{(k)}.$$\n",
    "The general standard deviation $\\sigma$ and pixel-wise standard deviation $\\sigma_{ij}$ is defined similarly.\n",
    "\n",
    "Which of the following preprocessing steps will not change the performance of a Nearest Neighbor classifier that uses L1 distance? Select all that apply.\n",
    "1. Subtracting the mean $\\mu$ ($\\tilde{p}_{ij}^{(k)}=p_{ij}^{(k)}-\\mu$.)\n",
    "2. Subtracting the per pixel mean $\\mu_{ij}$  ($\\tilde{p}_{ij}^{(k)}=p_{ij}^{(k)}-\\mu_{ij}$.)\n",
    "3. Subtracting the mean $\\mu$ and dividing by the standard deviation $\\sigma$.\n",
    "4. Subtracting the pixel-wise mean $\\mu_{ij}$ and dividing by the pixel-wise standard deviation $\\sigma_{ij}$.\n",
    "5. Rotating the coordinate axes of the data.\n",
    "\n",
    "$\\color{blue}{\\textit Your Answer:}$\n",
    "\n",
    "The options that will not change the performance of a Nearest Neighbor classifier that uses L1 distance are 1, 2, 3 and 5.\n",
    "\n",
    "\n",
    "$\\color{blue}{\\textit Your Explanation:}$\n",
    "\n",
    "Let's define L1 distance as $d_1(I_1, I_2) = \\sum_{i,j} |I_1(i,j) - I_2(i,j)|$.\n",
    "\n",
    "1. Subtracting the mean $\\mu$ to all pixel values will lead to this equation: $d_1(I_1, I_2) = \\sum_{i,j} |I_1(i,j) - \\mu - (I_2(i,j) - \\mu)|$ that simplifies to $d_1(I_1, I_2) = \\sum_{i,j} |I_1(i,j) - I_2(i,j)|$. This is the same as the original L1 distance, so the performance of the classifier will not change.\n",
    "\n",
    "2. Subtracting the per pixel mean $\\mu_{ij}$ to all pixel values will lead to this equation: $d_1(I_1, I_2) = \\sum_{i,j} |I_1(i,j) - \\mu_{ij} - (I_2(i,j) - \\mu_{ij})|$ that simplifies to $d_1(I_1, I_2) = \\sum_{i,j} |I_1(i,j) - I_2(i,j)|$. This is the same as the original L1 distance, so the performance of the classifier will not change.\n",
    "\n",
    "3. Subtracting the mean $\\mu$ and dividing by the standard deviation $\\sigma$ will lead to this equation: $d_1(I_1, I_2) = \\sum_{i,j} |\\frac{I_1(i,j) - \\mu}{\\sigma} - \\frac{I_2(i,j) - \\mu}{\\sigma}|$ that simplifies to $d_1(I_1, I_2) = \\sum_{i,j} |\\frac{I_1(i,j) - I_2(i,j)}{\\sigma}| = \\frac{1}{\\sigma} \\sum_{i,j} |I_1(i,j) - I_2(i,j)|$. Now because in KNN we only are comparing distances, the factor $\\frac{1}{\\sigma}$ will be canceled out when comparing distances, so the performance of the classifier will not change.\n",
    "\n",
    "4. Subtracting the pixel-wise mean $\\mu_{ij}$ and dividing by the pixel-wise standard deviation $\\sigma_{ij}$ will lead to this equation: $d_1(I_1, I_2) = \\sum_{i,j} |\\frac{I_1(i,j) - \\mu_{ij}}{\\sigma_{ij}} - \\frac{I_2(i,j) - \\mu_{ij}}{\\sigma_{ij}}| =  \\sum_{i,j} |\\frac{I_1(i,j) - I_2(i,j)}{\\sigma_{ij}}|$. Due to different standard deviations in each pixel, some pixels will have more weight than others, this could lead to a change in comparison of distances, so the performance of the classifier will change.\n",
    "\n",
    "5. Rotating the coordinate axes of the data will not change the performance of the classifier, because the L1 distance is invariant to rotation of the coordinate axes.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3\n",
    "\n",
    "Which of the following statements about $k$-Nearest Neighbor ($k$-NN) are true in a classification setting, and for all $k$? Select all that apply.\n",
    "1. The decision boundary (hyperplane between classes in feature space) of the k-NN classifier is linear.\n",
    "2. The training error of a 1-NN will always be lower than that of 5-NN.\n",
    "3. The test error of a 1-NN will always be lower than that of a 5-NN.\n",
    "4. The time needed to classify a test example with the k-NN classifier grows with the size of the training set.\n",
    "5. None of the above.\n",
    "\n",
    "$\\color{blue}{\\textit Your Answer:}$\n",
    "\n",
    "The statements that are true are 2 and 4.\n",
    "\n",
    "$\\color{blue}{\\textit Your Explanation:}$\n",
    "\n",
    "1. The decision boundary of the k-NN classifier is linear. This statement is false. The decision boundary of a k-NN classifier is not necessarily linear. The shape of the decision boundary depends on the distribution of the data and the value of $k$. For small values of $k$, the decision boundary can be very irregular, while for larger values of $k$, the decision boundary may appear smoother. However, it does not have to be linear.\n",
    "\n",
    "2. The training error of a 1-NN will always be lower than that of 5-NN. This statement is typically true. For a 1-NN classifier, the nearest neighbor of any point in the training set is the point itself, leading to a perfect classification on the training data (assuming no duplicate points with different labels), which means the training error is zero. As $k$ increases, the classifier considers more neighbors, which may include points from other classes, increasing the potential for training errors.\n",
    "\n",
    "3. The test error of a 1-NN will always be lower than that of a 5-NN. This statement is false. The test error of a 1-NN classifier compared to a 5-NN classifier can be higher or lower, depending on the data. A 1-NN classifier can be very sensitive to noise in the data, leading to higher test errors due to overfitting. A 5-NN classifier, by considering more neighbors, can potentially generalize better and reduce the impact of noise, possibly leading to lower test errors.\n",
    "\n",
    "4. The time needed to classify a test example with the k-NN classifier grows with the size of the training set. This statement is true. The computational complexity of classifying a test example with k-NN is directly related to the size of the training set because the algorithm needs to compute the distance of the test example to each point in the training set to identify the $k$ nearest neighbors. Thus, as the size of the training set increases, the time needed to classify a test example increases.\n",
    "\n",
    "5. None of the above. Since statements 2 and 4 are true, this option is incorrect.\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mipt",
   "language": "python",
   "name": "mipt"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
