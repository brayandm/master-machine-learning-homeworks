{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-86e0de040aac317a",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "# Lab assignment â„–1, part 2\n",
    "\n",
    "This lab assignment consists of several parts. You are supposed to make some transformations, train some models, estimate the quality of the models and explain your results.\n",
    "\n",
    "Several comments:\n",
    "* Don't hesitate to ask questions, it's a good practice.\n",
    "* No private/public sharing, please. The copied assignments will be graded with 0 points.\n",
    "* Blocks of this lab will be graded separately."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__*This is the second part of the assignment. First and third parts are waiting for you in the same directory.*__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-512ba712fc0fc065",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "## Part 2. Data preprocessing, model training and evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-b656a4266174b009",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "### 1. Reading the data\n",
    "Today we work with the [dataset](https://archive.ics.uci.edu/ml/datasets/Statlog+%28Vehicle+Silhouettes%29), describing different cars for multiclass ($k=4$) classification problem. The data is available below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If on colab, uncomment the following lines\n",
    "\n",
    "# ! wget https://raw.githubusercontent.com/girafe-ai/ml-course/22f_made/homeworks/lab01_ml_pipeline/car_data.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-eebac6bfdf73d0bc",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(846, 19) (846,)\n",
      "(549, 19) (549,) (297, 19) (297,)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "dataset = pd.read_csv('car_data.csv', delimiter=',', header=None).values\n",
    "data = dataset[:, :-1].astype(int)\n",
    "target = dataset[:, -1]\n",
    "\n",
    "print(data.shape, target.shape)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, target, test_size=0.35)\n",
    "print(X_train.shape, y_train.shape, X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-88b1a0f688568f2c",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "To get some insights about the dataset, `pandas` might be used. The `train` part is transformed to `pd.DataFrame` below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>607</td>\n",
       "      <td>86</td>\n",
       "      <td>39</td>\n",
       "      <td>62</td>\n",
       "      <td>129</td>\n",
       "      <td>59</td>\n",
       "      <td>6</td>\n",
       "      <td>116</td>\n",
       "      <td>57</td>\n",
       "      <td>17</td>\n",
       "      <td>135</td>\n",
       "      <td>137</td>\n",
       "      <td>203</td>\n",
       "      <td>145</td>\n",
       "      <td>64</td>\n",
       "      <td>7</td>\n",
       "      <td>9</td>\n",
       "      <td>199</td>\n",
       "      <td>204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>187</td>\n",
       "      <td>98</td>\n",
       "      <td>45</td>\n",
       "      <td>76</td>\n",
       "      <td>166</td>\n",
       "      <td>60</td>\n",
       "      <td>7</td>\n",
       "      <td>157</td>\n",
       "      <td>42</td>\n",
       "      <td>20</td>\n",
       "      <td>148</td>\n",
       "      <td>184</td>\n",
       "      <td>371</td>\n",
       "      <td>186</td>\n",
       "      <td>69</td>\n",
       "      <td>13</td>\n",
       "      <td>10</td>\n",
       "      <td>190</td>\n",
       "      <td>196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>366</td>\n",
       "      <td>90</td>\n",
       "      <td>47</td>\n",
       "      <td>85</td>\n",
       "      <td>149</td>\n",
       "      <td>60</td>\n",
       "      <td>10</td>\n",
       "      <td>155</td>\n",
       "      <td>43</td>\n",
       "      <td>19</td>\n",
       "      <td>155</td>\n",
       "      <td>179</td>\n",
       "      <td>355</td>\n",
       "      <td>186</td>\n",
       "      <td>75</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>185</td>\n",
       "      <td>196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>387</td>\n",
       "      <td>90</td>\n",
       "      <td>47</td>\n",
       "      <td>85</td>\n",
       "      <td>145</td>\n",
       "      <td>58</td>\n",
       "      <td>9</td>\n",
       "      <td>152</td>\n",
       "      <td>44</td>\n",
       "      <td>19</td>\n",
       "      <td>155</td>\n",
       "      <td>175</td>\n",
       "      <td>345</td>\n",
       "      <td>184</td>\n",
       "      <td>73</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>186</td>\n",
       "      <td>197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>837</td>\n",
       "      <td>94</td>\n",
       "      <td>46</td>\n",
       "      <td>77</td>\n",
       "      <td>169</td>\n",
       "      <td>60</td>\n",
       "      <td>8</td>\n",
       "      <td>158</td>\n",
       "      <td>42</td>\n",
       "      <td>20</td>\n",
       "      <td>148</td>\n",
       "      <td>181</td>\n",
       "      <td>373</td>\n",
       "      <td>181</td>\n",
       "      <td>67</td>\n",
       "      <td>12</td>\n",
       "      <td>2</td>\n",
       "      <td>193</td>\n",
       "      <td>199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>420</td>\n",
       "      <td>96</td>\n",
       "      <td>39</td>\n",
       "      <td>58</td>\n",
       "      <td>117</td>\n",
       "      <td>51</td>\n",
       "      <td>6</td>\n",
       "      <td>133</td>\n",
       "      <td>52</td>\n",
       "      <td>18</td>\n",
       "      <td>139</td>\n",
       "      <td>154</td>\n",
       "      <td>255</td>\n",
       "      <td>150</td>\n",
       "      <td>86</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>181</td>\n",
       "      <td>182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>641</td>\n",
       "      <td>84</td>\n",
       "      <td>43</td>\n",
       "      <td>76</td>\n",
       "      <td>180</td>\n",
       "      <td>75</td>\n",
       "      <td>7</td>\n",
       "      <td>155</td>\n",
       "      <td>43</td>\n",
       "      <td>19</td>\n",
       "      <td>143</td>\n",
       "      <td>180</td>\n",
       "      <td>359</td>\n",
       "      <td>173</td>\n",
       "      <td>77</td>\n",
       "      <td>5</td>\n",
       "      <td>12</td>\n",
       "      <td>185</td>\n",
       "      <td>190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>625</td>\n",
       "      <td>104</td>\n",
       "      <td>53</td>\n",
       "      <td>101</td>\n",
       "      <td>190</td>\n",
       "      <td>63</td>\n",
       "      <td>10</td>\n",
       "      <td>213</td>\n",
       "      <td>32</td>\n",
       "      <td>24</td>\n",
       "      <td>166</td>\n",
       "      <td>218</td>\n",
       "      <td>664</td>\n",
       "      <td>202</td>\n",
       "      <td>74</td>\n",
       "      <td>13</td>\n",
       "      <td>21</td>\n",
       "      <td>188</td>\n",
       "      <td>198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>552</td>\n",
       "      <td>90</td>\n",
       "      <td>39</td>\n",
       "      <td>89</td>\n",
       "      <td>181</td>\n",
       "      <td>62</td>\n",
       "      <td>8</td>\n",
       "      <td>175</td>\n",
       "      <td>38</td>\n",
       "      <td>21</td>\n",
       "      <td>132</td>\n",
       "      <td>200</td>\n",
       "      <td>458</td>\n",
       "      <td>154</td>\n",
       "      <td>70</td>\n",
       "      <td>11</td>\n",
       "      <td>15</td>\n",
       "      <td>189</td>\n",
       "      <td>195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>326</td>\n",
       "      <td>106</td>\n",
       "      <td>49</td>\n",
       "      <td>96</td>\n",
       "      <td>201</td>\n",
       "      <td>61</td>\n",
       "      <td>10</td>\n",
       "      <td>181</td>\n",
       "      <td>36</td>\n",
       "      <td>21</td>\n",
       "      <td>158</td>\n",
       "      <td>197</td>\n",
       "      <td>494</td>\n",
       "      <td>180</td>\n",
       "      <td>62</td>\n",
       "      <td>19</td>\n",
       "      <td>15</td>\n",
       "      <td>202</td>\n",
       "      <td>209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>396</td>\n",
       "      <td>108</td>\n",
       "      <td>57</td>\n",
       "      <td>106</td>\n",
       "      <td>177</td>\n",
       "      <td>51</td>\n",
       "      <td>5</td>\n",
       "      <td>256</td>\n",
       "      <td>26</td>\n",
       "      <td>28</td>\n",
       "      <td>170</td>\n",
       "      <td>285</td>\n",
       "      <td>966</td>\n",
       "      <td>261</td>\n",
       "      <td>87</td>\n",
       "      <td>11</td>\n",
       "      <td>2</td>\n",
       "      <td>182</td>\n",
       "      <td>181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>447</td>\n",
       "      <td>95</td>\n",
       "      <td>57</td>\n",
       "      <td>104</td>\n",
       "      <td>228</td>\n",
       "      <td>74</td>\n",
       "      <td>10</td>\n",
       "      <td>212</td>\n",
       "      <td>31</td>\n",
       "      <td>24</td>\n",
       "      <td>175</td>\n",
       "      <td>224</td>\n",
       "      <td>670</td>\n",
       "      <td>223</td>\n",
       "      <td>74</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>186</td>\n",
       "      <td>193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>444</td>\n",
       "      <td>102</td>\n",
       "      <td>53</td>\n",
       "      <td>101</td>\n",
       "      <td>238</td>\n",
       "      <td>72</td>\n",
       "      <td>4</td>\n",
       "      <td>238</td>\n",
       "      <td>28</td>\n",
       "      <td>26</td>\n",
       "      <td>163</td>\n",
       "      <td>267</td>\n",
       "      <td>844</td>\n",
       "      <td>242</td>\n",
       "      <td>85</td>\n",
       "      <td>7</td>\n",
       "      <td>22</td>\n",
       "      <td>184</td>\n",
       "      <td>184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>363</td>\n",
       "      <td>89</td>\n",
       "      <td>47</td>\n",
       "      <td>85</td>\n",
       "      <td>147</td>\n",
       "      <td>58</td>\n",
       "      <td>10</td>\n",
       "      <td>153</td>\n",
       "      <td>44</td>\n",
       "      <td>19</td>\n",
       "      <td>151</td>\n",
       "      <td>175</td>\n",
       "      <td>349</td>\n",
       "      <td>186</td>\n",
       "      <td>74</td>\n",
       "      <td>13</td>\n",
       "      <td>7</td>\n",
       "      <td>186</td>\n",
       "      <td>197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>693</td>\n",
       "      <td>90</td>\n",
       "      <td>46</td>\n",
       "      <td>75</td>\n",
       "      <td>133</td>\n",
       "      <td>55</td>\n",
       "      <td>11</td>\n",
       "      <td>160</td>\n",
       "      <td>43</td>\n",
       "      <td>20</td>\n",
       "      <td>161</td>\n",
       "      <td>173</td>\n",
       "      <td>369</td>\n",
       "      <td>171</td>\n",
       "      <td>77</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>182</td>\n",
       "      <td>192</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0    1   2    3    4   5   6    7   8   9    10   11   12   13  14  15  \\\n",
       "0   607   86  39   62  129  59   6  116  57  17  135  137  203  145  64   7   \n",
       "1   187   98  45   76  166  60   7  157  42  20  148  184  371  186  69  13   \n",
       "2   366   90  47   85  149  60  10  155  43  19  155  179  355  186  75   1   \n",
       "3   387   90  47   85  145  58   9  152  44  19  155  175  345  184  73   4   \n",
       "4   837   94  46   77  169  60   8  158  42  20  148  181  373  181  67  12   \n",
       "5   420   96  39   58  117  51   6  133  52  18  139  154  255  150  86   6   \n",
       "6   641   84  43   76  180  75   7  155  43  19  143  180  359  173  77   5   \n",
       "7   625  104  53  101  190  63  10  213  32  24  166  218  664  202  74  13   \n",
       "8   552   90  39   89  181  62   8  175  38  21  132  200  458  154  70  11   \n",
       "9   326  106  49   96  201  61  10  181  36  21  158  197  494  180  62  19   \n",
       "10  396  108  57  106  177  51   5  256  26  28  170  285  966  261  87  11   \n",
       "11  447   95  57  104  228  74  10  212  31  24  175  224  670  223  74   0   \n",
       "12  444  102  53  101  238  72   4  238  28  26  163  267  844  242  85   7   \n",
       "13  363   89  47   85  147  58  10  153  44  19  151  175  349  186  74  13   \n",
       "14  693   90  46   75  133  55  11  160  43  20  161  173  369  171  77   0   \n",
       "\n",
       "    16   17   18  \n",
       "0    9  199  204  \n",
       "1   10  190  196  \n",
       "2    5  185  196  \n",
       "3    2  186  197  \n",
       "4    2  193  199  \n",
       "5    0  181  182  \n",
       "6   12  185  190  \n",
       "7   21  188  198  \n",
       "8   15  189  195  \n",
       "9   15  202  209  \n",
       "10   2  182  181  \n",
       "11   4  186  193  \n",
       "12  22  184  184  \n",
       "13   7  186  197  \n",
       "14  16  182  192  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_pd = pd.DataFrame(X_train)\n",
    "\n",
    "# First 15 rows of our dataset.\n",
    "X_train_pd.head(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-98e7d91d77d65fcf",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "Methods `describe` and `info` deliver some useful information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>549.000000</td>\n",
       "      <td>549.000000</td>\n",
       "      <td>549.000000</td>\n",
       "      <td>549.000000</td>\n",
       "      <td>549.000000</td>\n",
       "      <td>549.000000</td>\n",
       "      <td>549.000000</td>\n",
       "      <td>549.000000</td>\n",
       "      <td>549.000000</td>\n",
       "      <td>549.000000</td>\n",
       "      <td>549.000000</td>\n",
       "      <td>549.000000</td>\n",
       "      <td>549.000000</td>\n",
       "      <td>549.000000</td>\n",
       "      <td>549.000000</td>\n",
       "      <td>549.000000</td>\n",
       "      <td>549.000000</td>\n",
       "      <td>549.000000</td>\n",
       "      <td>549.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>407.491803</td>\n",
       "      <td>93.493625</td>\n",
       "      <td>44.841530</td>\n",
       "      <td>81.814208</td>\n",
       "      <td>169.224044</td>\n",
       "      <td>61.956284</td>\n",
       "      <td>8.706740</td>\n",
       "      <td>168.275046</td>\n",
       "      <td>41.023679</td>\n",
       "      <td>20.553734</td>\n",
       "      <td>148.151184</td>\n",
       "      <td>188.387978</td>\n",
       "      <td>436.435337</td>\n",
       "      <td>174.143898</td>\n",
       "      <td>72.566485</td>\n",
       "      <td>6.118397</td>\n",
       "      <td>12.610200</td>\n",
       "      <td>188.908925</td>\n",
       "      <td>195.693989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>242.152856</td>\n",
       "      <td>8.091316</td>\n",
       "      <td>6.141758</td>\n",
       "      <td>15.610978</td>\n",
       "      <td>34.068615</td>\n",
       "      <td>8.478284</td>\n",
       "      <td>4.870922</td>\n",
       "      <td>32.664951</td>\n",
       "      <td>7.656004</td>\n",
       "      <td>2.555110</td>\n",
       "      <td>14.419197</td>\n",
       "      <td>30.915955</td>\n",
       "      <td>173.531518</td>\n",
       "      <td>32.115997</td>\n",
       "      <td>7.915017</td>\n",
       "      <td>4.813550</td>\n",
       "      <td>9.027996</td>\n",
       "      <td>6.215334</td>\n",
       "      <td>7.414302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>77.000000</td>\n",
       "      <td>33.000000</td>\n",
       "      <td>40.000000</td>\n",
       "      <td>104.000000</td>\n",
       "      <td>47.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>112.000000</td>\n",
       "      <td>26.000000</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>118.000000</td>\n",
       "      <td>130.000000</td>\n",
       "      <td>184.000000</td>\n",
       "      <td>109.000000</td>\n",
       "      <td>60.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>176.000000</td>\n",
       "      <td>181.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>198.000000</td>\n",
       "      <td>87.000000</td>\n",
       "      <td>40.000000</td>\n",
       "      <td>70.000000</td>\n",
       "      <td>141.000000</td>\n",
       "      <td>57.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>147.000000</td>\n",
       "      <td>33.000000</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>137.000000</td>\n",
       "      <td>168.000000</td>\n",
       "      <td>319.000000</td>\n",
       "      <td>149.000000</td>\n",
       "      <td>67.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>184.000000</td>\n",
       "      <td>191.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>403.000000</td>\n",
       "      <td>93.000000</td>\n",
       "      <td>44.000000</td>\n",
       "      <td>80.000000</td>\n",
       "      <td>166.000000</td>\n",
       "      <td>61.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>157.000000</td>\n",
       "      <td>43.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>146.000000</td>\n",
       "      <td>178.000000</td>\n",
       "      <td>363.000000</td>\n",
       "      <td>174.000000</td>\n",
       "      <td>72.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>189.000000</td>\n",
       "      <td>196.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>612.000000</td>\n",
       "      <td>99.000000</td>\n",
       "      <td>49.000000</td>\n",
       "      <td>96.000000</td>\n",
       "      <td>194.000000</td>\n",
       "      <td>65.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>197.000000</td>\n",
       "      <td>46.000000</td>\n",
       "      <td>23.000000</td>\n",
       "      <td>159.000000</td>\n",
       "      <td>215.000000</td>\n",
       "      <td>579.000000</td>\n",
       "      <td>197.000000</td>\n",
       "      <td>76.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>193.000000</td>\n",
       "      <td>201.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>842.000000</td>\n",
       "      <td>119.000000</td>\n",
       "      <td>59.000000</td>\n",
       "      <td>112.000000</td>\n",
       "      <td>333.000000</td>\n",
       "      <td>138.000000</td>\n",
       "      <td>52.000000</td>\n",
       "      <td>265.000000</td>\n",
       "      <td>61.000000</td>\n",
       "      <td>29.000000</td>\n",
       "      <td>186.000000</td>\n",
       "      <td>320.000000</td>\n",
       "      <td>1018.000000</td>\n",
       "      <td>264.000000</td>\n",
       "      <td>135.000000</td>\n",
       "      <td>22.000000</td>\n",
       "      <td>41.000000</td>\n",
       "      <td>206.000000</td>\n",
       "      <td>211.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               0           1           2           3           4           5   \\\n",
       "count  549.000000  549.000000  549.000000  549.000000  549.000000  549.000000   \n",
       "mean   407.491803   93.493625   44.841530   81.814208  169.224044   61.956284   \n",
       "std    242.152856    8.091316    6.141758   15.610978   34.068615    8.478284   \n",
       "min      0.000000   77.000000   33.000000   40.000000  104.000000   47.000000   \n",
       "25%    198.000000   87.000000   40.000000   70.000000  141.000000   57.000000   \n",
       "50%    403.000000   93.000000   44.000000   80.000000  166.000000   61.000000   \n",
       "75%    612.000000   99.000000   49.000000   96.000000  194.000000   65.000000   \n",
       "max    842.000000  119.000000   59.000000  112.000000  333.000000  138.000000   \n",
       "\n",
       "               6           7           8           9           10          11  \\\n",
       "count  549.000000  549.000000  549.000000  549.000000  549.000000  549.000000   \n",
       "mean     8.706740  168.275046   41.023679   20.553734  148.151184  188.387978   \n",
       "std      4.870922   32.664951    7.656004    2.555110   14.419197   30.915955   \n",
       "min      2.000000  112.000000   26.000000   17.000000  118.000000  130.000000   \n",
       "25%      7.000000  147.000000   33.000000   19.000000  137.000000  168.000000   \n",
       "50%      8.000000  157.000000   43.000000   20.000000  146.000000  178.000000   \n",
       "75%     10.000000  197.000000   46.000000   23.000000  159.000000  215.000000   \n",
       "max     52.000000  265.000000   61.000000   29.000000  186.000000  320.000000   \n",
       "\n",
       "                12          13          14          15          16  \\\n",
       "count   549.000000  549.000000  549.000000  549.000000  549.000000   \n",
       "mean    436.435337  174.143898   72.566485    6.118397   12.610200   \n",
       "std     173.531518   32.115997    7.915017    4.813550    9.027996   \n",
       "min     184.000000  109.000000   60.000000    0.000000    0.000000   \n",
       "25%     319.000000  149.000000   67.000000    2.000000    6.000000   \n",
       "50%     363.000000  174.000000   72.000000    5.000000   11.000000   \n",
       "75%     579.000000  197.000000   76.000000    9.000000   19.000000   \n",
       "max    1018.000000  264.000000  135.000000   22.000000   41.000000   \n",
       "\n",
       "               17          18  \n",
       "count  549.000000  549.000000  \n",
       "mean   188.908925  195.693989  \n",
       "std      6.215334    7.414302  \n",
       "min    176.000000  181.000000  \n",
       "25%    184.000000  191.000000  \n",
       "50%    189.000000  196.000000  \n",
       "75%    193.000000  201.000000  \n",
       "max    206.000000  211.000000  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_pd.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 549 entries, 0 to 548\n",
      "Data columns (total 19 columns):\n",
      " #   Column  Non-Null Count  Dtype\n",
      "---  ------  --------------  -----\n",
      " 0   0       549 non-null    int64\n",
      " 1   1       549 non-null    int64\n",
      " 2   2       549 non-null    int64\n",
      " 3   3       549 non-null    int64\n",
      " 4   4       549 non-null    int64\n",
      " 5   5       549 non-null    int64\n",
      " 6   6       549 non-null    int64\n",
      " 7   7       549 non-null    int64\n",
      " 8   8       549 non-null    int64\n",
      " 9   9       549 non-null    int64\n",
      " 10  10      549 non-null    int64\n",
      " 11  11      549 non-null    int64\n",
      " 12  12      549 non-null    int64\n",
      " 13  13      549 non-null    int64\n",
      " 14  14      549 non-null    int64\n",
      " 15  15      549 non-null    int64\n",
      " 16  16      549 non-null    int64\n",
      " 17  17      549 non-null    int64\n",
      " 18  18      549 non-null    int64\n",
      "dtypes: int64(19)\n",
      "memory usage: 81.6 KB\n"
     ]
    }
   ],
   "source": [
    "X_train_pd.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-be844269be69c387",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "### 2. Machine Learning pipeline\n",
    "Here you are supposed to perform the desired transformations. Please, explain your results briefly after each task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.0. Data preprocessing\n",
    "* Make some transformations of the dataset (if necessary). Briefly explain the transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-a1514aa189a49fca",
     "locked": false,
     "points": 15,
     "schema_version": 2,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "### YOUR CODE HERE\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# The data is scaled to have a mean of 0 and a variance of 1 so\n",
    "# that the model can converge faster.\n",
    "# Because we dont have null values in our dataset and all are\n",
    "# numerical features we dont need to do nothing else.\n",
    "pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()), \n",
    "])\n",
    "\n",
    "X_train_transformed = pipeline.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1. Basic logistic regression\n",
    "* Find optimal hyperparameters for logistic regression with cross-validation on the `train` data (small grid/random search is enough, no need to find the *best* parameters).\n",
    "\n",
    "* Estimate the model quality with `f1` and `accuracy` scores.\n",
    "* Plot a ROC-curve for the trained model. For the multiclass case you might use `scikitplot` library (e.g. `scikitplot.metrics.plot_roc(test_labels, predicted_proba)`).\n",
    "\n",
    "*Note: please, use the following hyperparameters for logistic regression: `multi_class='multinomial'`, `solver='saga'` `tol=1e-3` and ` max_iter=500`.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-1dd5ad5d0845cbbb",
     "locked": false,
     "points": 5,
     "schema_version": 2,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "### YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You might use this command to install scikit-plot. \n",
    "# Warning, if you a running locally, don't call pip from within jupyter, call it from terminal in the corresponding \n",
    "# virtual environment instead\n",
    "\n",
    "# ! pip install scikit-plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2. PCA: explained variance plot\n",
    "* Apply the PCA to the train part of the data. Build the explaided variance plot. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-c6c614740bce090e",
     "locked": false,
     "points": 10,
     "schema_version": 2,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "### YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-0c1fe666f52fe53c",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "#### 2.3. PCA trasformation\n",
    "* Select the appropriate number of components. Briefly explain your choice. Should you normalize the data?\n",
    "\n",
    "*Use `fit` and `transform` methods to transform the `train` and `test` parts.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-96ab18d96473ef71",
     "locked": false,
     "points": 5,
     "schema_version": 2,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "### YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note: From this point `sklearn` [Pipeline](https://scikit-learn.org/stable/modules/compose.html) might be useful to perform transformations on the data. Refer to the [docs](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html) for more information.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-d28b58a35c94e988",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "#### 2.4. Logistic regression on PCA-preprocessed data.\n",
    "* Find optimal hyperparameters for logistic regression with cross-validation on the transformed by PCA `train` data.\n",
    "\n",
    "* Estimate the model quality with `f1` and `accuracy` scores.\n",
    "* Plot a ROC-curve for the trained model. For the multiclass case you might use `scikitplot` library (e.g. `scikitplot.metrics.plot_roc(test_labels, predicted_proba)`).\n",
    "\n",
    "*Note: please, use the following hyperparameters for logistic regression: `multi_class='multinomial'`, `solver='saga'` and `tol=1e-3`*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-12d53ea45258fa82",
     "locked": false,
     "points": 5,
     "schema_version": 2,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "### YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-4fbf16c64076e139",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "#### 2.5. Decision tree\n",
    "* Now train a desicion tree on the same data. Find optimal tree depth (`max_depth`) using cross-validation.\n",
    "\n",
    "* Measure the model quality using the same metrics you used above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-748ed20b51c67fab",
     "locked": false,
     "points": 15,
     "schema_version": 2,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-9eadd4d8a03ae67a",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "#### 2.6. Bagging.\n",
    "Here starts the ensembling part.\n",
    "\n",
    "First we will use the __Bagging__ approach. Build an ensemble of $N$ algorithms varying N from $N_{min}=2$ to $N_{max}=100$ (with step 5).\n",
    "\n",
    "We will build two ensembles: of logistic regressions and of decision trees.\n",
    "\n",
    "*Comment: each ensemble should be constructed from models of the same family, so logistic regressions should not be mixed up with decision trees.*\n",
    "\n",
    "\n",
    "*Hint 1: To build a __Bagging__ ensebmle varying the ensemble size efficiently you might generate $N_{max}$ subsets of `train` data (of the same size as the original dataset) using bootstrap procedure once. Then you train a new instance of logistic regression/decision tree with optimal hyperparameters you estimated before on each subset (so you train it from scratch). Finally, to get an ensemble of $N$ models you average the $N$ out of $N_{max}$ models predictions.*\n",
    "\n",
    "*Hint 2: sklearn might help you with this taks. Some appropriate function/class might be out there.*\n",
    "\n",
    "* Plot `f1` and `accuracy` scores plots w.r.t. the size of the ensemble.\n",
    "\n",
    "* Briefly analyse the plot. What is the optimal number of algorithms? Explain your answer.\n",
    "\n",
    "* How do you think, are the hyperparameters for the decision trees you found in 2.5 optimal for trees used in ensemble? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-8fc95a2b206bdae1",
     "locked": false,
     "points": 35,
     "schema_version": 2,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-241b7691ab44cbfb",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "#### 2.7. Random Forest\n",
    "Now we will work with the Random Forest (its `sklearn` implementation).\n",
    "\n",
    "* * Plot `f1` and `accuracy` scores plots w.r.t. the number of trees in Random Forest.\n",
    "\n",
    "* What is the optimal number of trees you've got? Is it different from the optimal number of logistic regressions/decision trees in 2.6? Explain the results briefly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-888755d0f3d91620",
     "locked": false,
     "points": 15,
     "schema_version": 2,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-99191c0852538d4d",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "#### 2.8. Learning curve\n",
    "Your goal is to estimate, how does the model behaviour change with the increase of the `train` dataset size.\n",
    "\n",
    "* Split the training data into 10 equal (almost) parts. Then train the models from above (Logistic regression, Desicion Tree, Random Forest) with optimal hyperparameters you have selected on 1 part, 2 parts (combined, so the train size in increased by 2 times), 3 parts and so on.\n",
    "\n",
    "* Build a plot of `accuracy` and `f1` scores on `test` part, varying the `train` dataset size (so the axes will be score - dataset size.\n",
    "\n",
    "* Analyse the final plot. Can you make any conlusions using it? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-e39bc7e7dff61ff9",
     "locked": false,
     "points": 15,
     "schema_version": 2,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.9. Boosting\n",
    "Your goal is to build a boosting ensemble using xgboost, CatBoost or lightgbm package.\n",
    "Please, do not use the sklearn API for these models.\n",
    "\n",
    "Find optimal number of decision trees in the boosting ensembe using grid search or other methods.\n",
    "Please, explain your answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Create Assignment",
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
